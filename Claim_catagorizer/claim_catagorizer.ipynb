{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a52048",
   "metadata": {},
   "source": [
    "Made by Pieter de Jong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b9e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from difflib import ndiff, get_close_matches\n",
    "import textdistance\n",
    "import string\n",
    "import json\n",
    "import io\n",
    "import os\n",
    "import panel as pn\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0094a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def startupCheck():\n",
    "    if os.path.isfile(\"claim_matching.json\") and os.access(\"claim_matching.json\", os.R_OK):\n",
    "        # checks if file exists\n",
    "        print (\"File found\")\n",
    "    else:\n",
    "        print (\"Creating file\")\n",
    "        with io.open(os.path.join(\"\", 'claim_matching.json'), 'w') as db_file:\n",
    "            db_file.write(json.dumps({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceba347",
   "metadata": {},
   "outputs": [],
   "source": [
    "startupCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88cfb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"C:\\\\Users\\\\piete\\\\AppData\\\\Roaming\\\\MobaXterm\\\\slash\\\\RemoteFiles\\\\2099638_2_1\\\\2020-2022_BAK Cakes&Sweet_Goods_WE1 (8300).xls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d78fd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take Event Date, Product Name and Claims/Features from all sheets in the excel file\n",
    "df = pd.concat(pd.read_excel(filename, usecols=[\"Event Date\", \"Product Name\", \"Claims/Features\"], sheet_name=None), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dc8d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/commons/dsls/fine_bakery/Data/\"\"\n",
    "#files = os.listdir(path)\n",
    "#files_xls = [f for f in files if f[-3:] == 'xls']\n",
    "#df = pd.DataFrame()\n",
    "#for f in files_xls:\n",
    "#    data = pd.read_excel(f, usecols=[\"Event Date\", \"Product Name\", \"Claims/Features\"], sheet_name=None), ignore_index=True\n",
    "#    df = df.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0148752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bd35de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drops rows without claims\n",
    "df = df.dropna(subset=[\"Claims/Features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ecaca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make all claims lowercase\n",
    "df[\"Claims/Features\"] = df[\"Claims/Features\"].str.lower()\n",
    "#Cleaning by renaming and removing some characters\n",
    "df[\"Claims/Features\"] = df[\"Claims/Features\"].str.replace(\",\", \".\").str.replace(\"\\n\", \" \").str.replace(\"\\'s\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f4346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove last dot in each claims/features. then turn the claims/features into a list of claims by splitting the content by the dots\n",
    "df[\"claims_proccesed\"] = df[\"Claims/Features\"].str.rstrip(\".\").str.split(\"\\. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f5b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd6054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that returns claims containing provided pattern\n",
    "def find_pattern(pattern, string):\n",
    "    return bool(re.search(pattern, string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2987d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing claims that contain patterns marking them non claims\n",
    "all_prod_claims = []\n",
    "pattern = \": \\d|kcal|kj|\\dg|\\d g|.org\"\n",
    "for claims in df[\"claims_proccesed\"]:\n",
    "    claims_no_ingredients = []\n",
    "    for claim in claims:\n",
    "        claim = claim.lstrip()\n",
    "        if not find_pattern(pattern, claim):\n",
    "            claims_no_ingredients.append(claim)\n",
    "    all_prod_claims.append(claims_no_ingredients)\n",
    "df[\"claims_proccesed\"] = all_prod_claims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e092048d",
   "metadata": {},
   "source": [
    "pattern = \": \\d|kcal|kj|\\dg|\\d g|.org\"\n",
    "claims_no_ingredients = []\n",
    "for claim in all_claims:\n",
    "    if not find_pattern(pattern, claim):\n",
    "        claims_no_ingredients.append(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d9716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c32ca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting sentences where a space was missing. including when next sentence starts with a number.\n",
    "#does not split abreviations like h.u.v and ignores no.1\n",
    "\n",
    "pattern = \"\\D\\.\\D\"\n",
    "pattern2 = \"\\D\\.\\D\\.\"\n",
    "pattern3 = \"\\D\\.\\d\"\n",
    "pattern4 = \"no.1\"\n",
    "claims_cleaned = []\n",
    "for claims in df[\"claims_proccesed\"]:\n",
    "    temp_claims = claims\n",
    "    for claim in claims:\n",
    "        if find_pattern(pattern, claim) and not find_pattern(pattern2, claim):\n",
    "            temp_claims.remove(claim)\n",
    "            temp_claims.append(claim.split(\".\")[0])\n",
    "            temp_claims.append(claim.split(\".\")[1])\n",
    "        \n",
    "\n",
    "        if find_pattern(pattern3, claim) and not find_pattern(pattern4, claim):\n",
    "            temp_claims.remove(claim)\n",
    "            temp_claims.append(claim.split(\".\")[0])\n",
    "            temp_claims.append(claim.split(\".\")[1])\n",
    "    claims_cleaned.append(temp_claims)\n",
    "df[\"claims_proccesed\"] = claims_cleaned\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4356ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of all unique claims and a list of all claims\n",
    "all_cleaned_unique_claims = []\n",
    "all_cleaned_claims = []\n",
    "for claims in df[\"claims_proccesed\"]:\n",
    "    for claim in claims:\n",
    "        all_cleaned_claims.append(claim)\n",
    "        if claim not in all_cleaned_unique_claims:\n",
    "            all_cleaned_unique_claims.append(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684e1fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Claim_ammount = Counter(all_cleaned_claims)\n",
    "Claim_ammount.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c5a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all claims containing pattern and adding these claims to claim_dict with pattern as key if their tickbox is selected\n",
    "claim_dict = {}\n",
    "pattern = \"vegetarian\"\n",
    "def get_matches(pattern):\n",
    "    #pattern = \"vegan\"\n",
    "    pattern_match = []\n",
    "    for claim in all_cleaned_unique_claims:\n",
    "        if find_pattern(pattern, claim):\n",
    "            if claim not in pattern_match:\n",
    "                pattern_match.append(claim)\n",
    "    return pattern_match\n",
    "\n",
    "\n",
    "#text_input = pn.widgets.TextInput(name=\"Claim search\", placeholder=\"Enter claim here\")\n",
    "checkbox_group = pn.widgets.CheckBoxGroup(name=\"Checkbox Group\", value=get_matches(pattern), options=get_matches(pattern))\n",
    "#column = pn.Column(text_input, checkbox_group)\n",
    "#column\n",
    "checkbox_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba44f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_dict[pattern] = checkbox_group.value\n",
    "\n",
    "filename = \"claim_matching.json\"\n",
    "\n",
    "    \n",
    "with open(filename, \"r+\") as jsonfile:\n",
    "    dic = json.load(jsonfile)\n",
    "    \n",
    "    for key in claim_dict.keys():\n",
    "        dic[key] = claim_dict[key]\n",
    "\n",
    "with open(filename, \"w\") as jsonfile:\n",
    "    json.dump(dic, jsonfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c1740",
   "metadata": {},
   "outputs": [],
   "source": [
    "for claims in df[\"claims_proccesed\"]:\n",
    "    print(any((True for x in dic[\"vegan\"] if x in claims)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f33ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ebd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4b6083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbcd224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d9138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_close_matches(\"recyclable\", [claim for claim in claims_no_ingredients], 10, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7af846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e29f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wnl = WordNetLemmatizer()\n",
    " \n",
    "# Define function to lemmatize each word with its POS tag\n",
    " \n",
    "# POS_TAGGER_FUNCTION : TYPE 1\n",
    "#def pos_tagger(nltk_tag):\n",
    "#    if nltk_tag.startswith('J'):\n",
    "#        return wordnet.ADJ\n",
    "#    elif nltk_tag.startswith('V'):\n",
    "#        return wordnet.VERB\n",
    "#    elif nltk_tag.startswith('N'):\n",
    "#        return wordnet.NOUN\n",
    "#    elif nltk_tag.startswith('R'):\n",
    "#        return wordnet.ADV\n",
    "#    else:         \n",
    "#        return None\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a98f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#claims_lemmatized = []\n",
    "#for sentence in claims_no_ingredients:\n",
    "#    pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "#    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "#    lemmatized_sentence = []\n",
    "#    for word, tag in wordnet_tagged:\n",
    "#        if tag is None:\n",
    "#            # if there is no available tag, append the token as is\n",
    "#            lemmatized_sentence.append(word)\n",
    "#        else:       \n",
    "#            # else use the tag to lemmatize the token\n",
    "#            lemmatized_sentence.append(wnl.lemmatize(word, tag))\n",
    "#    lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "# \n",
    "#    claims_lemmatized.append(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8a45df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c28e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Claim_ammount = Counter(claims_lemmatized)\n",
    "#Claim_ammount.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b28bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_close_matches(\"recyclable\", [claim for claim in claims_no_ingredients], 10, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552d575e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60996b88",
   "metadata": {},
   "source": [
    "To do:\n",
    "    spelling checker to fix all the misspelled claims\n",
    "    use difflib to find simmilar sentences?\n",
    "    or https://stackoverflow.com/questions/63718559/finding-most-similar-sentences-among-all-in-python\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed7e5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2258d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8012f92d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33acbbb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e0481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b2a39a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d0883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6a3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e35564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#short_claims = []\n",
    "#for claim in claims_no_ingredients:\n",
    "#    if len(claim) < 5:\n",
    "#        short_claims.append(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e0afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#long_claims = []\n",
    "#for claim in claims_no_ingredients:\n",
    "#    if len(claim) > 30:\n",
    "#        long_claims.append(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41881d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73a0b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4852b7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c771983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c548fb64",
   "metadata": {},
   "source": [
    "first make manual list\n",
    "\n",
    "\n",
    "#bert\n",
    "\n",
    "\n",
    "NMF unsupervised clustering method\n",
    "\n",
    "\n",
    "TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694e987",
   "metadata": {},
   "source": [
    "Manual list:\n",
    "vegetarian\n",
    "green dot certified\n",
    "recyclable\n",
    "christmas\n",
    "easter\n",
    "valentine's day\n",
    "king's day\n",
    "gluten free\n",
    "microwaveable\n",
    "palm oil free\n",
    "cocoa certified\n",
    "nutriscore c\n",
    "nutriscore d\n",
    "nutriscore e\n",
    "gmo free\n",
    "fsc certified\n",
    "vegan\n",
    "lactose free\n",
    "no preservatives\n",
    "no artificial colors\n",
    "no artificial flavors\n",
    "certified organic\n",
    "certified halal\n",
    "fairtrade cocoa\n",
    "free range eggs\n",
    "milk free"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dede39",
   "metadata": {},
   "source": [
    "manual_list = [\"vegetarian\",\n",
    "\"green dot certified\",\n",
    "\"recyclable\",\n",
    "\"christmas\",\n",
    "\"easter\",\n",
    "\"valentine's day\",\n",
    "\"king's day\",\n",
    "\"gluten free\",\n",
    "\"microwaveable\",\n",
    "\"palm oil free\",\n",
    "\"cocoa certified\",\n",
    "\"nutriscore c\",\n",
    "\"nutriscore d\",\n",
    "\"nutriscore e\",\n",
    "\"gmo free\",\n",
    "\"fsc certified\",\n",
    "\"vegan\",\n",
    "\"lactose free\",\n",
    "\"no preservatives\",\n",
    "\"no artificial colors\",\n",
    "\"no artificial flavors\",\n",
    "\"certified organic\",\n",
    "\"certified halal\",\n",
    "\"fairtrade cocoa\",\n",
    "\"free range eggs\",\n",
    "\"milk free\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76ebbf9",
   "metadata": {},
   "source": [
    "manual_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fe54cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f713c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

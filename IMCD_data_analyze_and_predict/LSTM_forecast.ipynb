{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164cd0c1",
   "metadata": {},
   "source": [
    "Let's use RNN (LSTM) to forecast the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_config():\n",
    "    with open(\"config.yaml\", 'r') as stream:\n",
    "        config = yaml.safe_load(stream)\n",
    "    return config\n",
    "\n",
    "config = get_config()\n",
    "dir_to = config['directory_to']\n",
    "file_name_to = config['cleared_data_to']\n",
    "cleared_data = pd.read_csv(os.path.join(dir_to, file_name_to), parse_dates=['Event Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720e53de",
   "metadata": {},
   "source": [
    "Let's prepare data for the prediction of user demand on the regional level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c265d21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_data = cleared_data[['Event Date', 'Sub-Category', 'Event', 'Region']].dropna(how='any')\n",
    "products_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939c1efc",
   "metadata": {},
   "source": [
    "Let's prepare data for the prediction of average category prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5637b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_price_prediction = cleared_data[['Event Date', 'Sub-Category', 'Event', 'Euro Price/Kg']].dropna(how='any')\n",
    "data_price_prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1651c36",
   "metadata": {},
   "source": [
    "Let's create function which return the dataset for making prediction. There are two possible types of prediction (prediction parameter) - demand and price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c19ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_time_series_analysis(cleared_data, event_type='all', category='all', region='all', prediction='demand'):\n",
    "    if event_type != 'all':\n",
    "        cleared_data  = cleared_data[cleared_data['Event'] == event_type]\n",
    "    if category != 'all':\n",
    "        cleared_data  = cleared_data[cleared_data['Sub-Category'] == category]\n",
    "    cleared_data = cleared_data.drop(columns=['Event', 'Sub-Category'])\n",
    "    if prediction == 'demand':\n",
    "        if region != 'all':\n",
    "            cleared_data = cleared_data[cleared_data['Region'] == region]\n",
    "        cleared_data = cleared_data.groupby('Event Date')['Region'].count()\n",
    "        cleared_data = cleared_data.reset_index()\n",
    "        cleared_data = cleared_data.rename(columns = {'Region':'Launches'})\n",
    "    elif prediction == 'price':\n",
    "        cleared_data = cleared_data.groupby(['Event Date']).mean()\n",
    "        cleared_data = cleared_data.reset_index()\n",
    "        cleared_data = cleared_data.rename(columns = {'Euro Price/Kg':'Prices'})\n",
    "    else:\n",
    "        raise Exception(\"Wrong prediction type. Use prediction=demand or prediction=price\")\n",
    "    return cleared_data[:-2], cleared_data['Event Date'][:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78324f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_demand, dates_demand = get_data_for_time_series_analysis(products_data, 'New Product', 'Cakes - Pastries & Sweet Goods', 'West Europe', 'demand')\n",
    "y_demand, dates_demand = get_data_for_time_series_analysis(products_data, 'New Product', 'Bread & Bread Products', 'West Europe', 'demand')\n",
    "y_demand.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_price, dates_price = get_data_for_time_series_analysis(data_price_prediction, prediction='price')\n",
    "y_price.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c299e3",
   "metadata": {},
   "source": [
    "The number of dates and actual values should be equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59aaa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_demand), len(dates_demand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b9626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter  \n",
    "\n",
    "def plot_df(x, y, title=\"\", xlabel='Date', ylabel='Value'):\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.plot(x, y, color='tab:red', label='Actual')\n",
    "    \n",
    "    yhat = savgol_filter(y, 9, 3)\n",
    "    plt.plot(x, yhat, color='green', linestyle='dashed', label='Smoothing')\n",
    "\n",
    "    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "plot_df(dates_demand, y_demand['Launches'], title='Launches between 2019 and 2023')  \n",
    "plot_df(dates_price, y_price['Prices'], title='Average prices between 2019 and 2023')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b29bbb",
   "metadata": {},
   "source": [
    "Let's create a Forecaster object. We can see that the start date for the analysis is 01.06.2019, the end data is 01.01.2023 with the monthly frequency. There are 44 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723da71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scalecast.Forecaster import Forecaster\n",
    "\n",
    "y = y_demand['Launches'].values\n",
    "\n",
    "f_demand = Forecaster(\n",
    "    y=y,\n",
    "    current_dates=dates_demand,\n",
    "    cis=True\n",
    ")\n",
    "f_demand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a44a5eb",
   "metadata": {},
   "source": [
    "Same characteristics are observed for the average prices dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf38de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scalecast.Forecaster import Forecaster\n",
    "\n",
    "f_price = Forecaster(\n",
    "    y=y_price['Prices'],\n",
    "    current_dates=dates_price\n",
    ")\n",
    "f_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc433bf8",
   "metadata": {},
   "source": [
    "We use the PACF plot to inspect partial correlations between the actual values and their lags (lags - previous observations, partial means wihout considering any lags inbetween). The plot_pacf function can only compute partial correlations for lags up to 50% of the sample size. So the lags parameter in the function call should be less than 22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e31761",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_demand.plot_pacf(lags=21, method='ywm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bda330",
   "metadata": {},
   "source": [
    "From this plot, it looks like some statistically significant correlations exist between the current and previous observations in the user demand dataset (for lags 1,4,5,14)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_price.plot_pacf(lags=21, method='ywm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e64ab",
   "metadata": {},
   "source": [
    "No statistically significant correlation exists between the current and previous observations in the price dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12db1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_demand = f_demand.seasonal_decompose()\n",
    "\n",
    "def plot_seasonal_decompose(res):\n",
    "    fig, (ax1,ax2,ax3) = plt.subplots(3,1, figsize=(18,10))\n",
    "    res.trend.plot(ax=ax1, title='Trend')\n",
    "    res.resid.plot(ax=ax2, title='Residuals')\n",
    "    res.seasonal.plot(ax=ax3, title='Seasonal pattern')\n",
    "    plt.show()\n",
    "    \n",
    "plot_seasonal_decompose(res_demand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76977dfb",
   "metadata": {},
   "source": [
    "There is no linear trend, but we do see a strong seasonality in the user demand dataset! The residuals do not  follow any pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf03200",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_price = f_price.seasonal_decompose()\n",
    "plot_seasonal_decompose(res_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba4d439",
   "metadata": {},
   "source": [
    "We don't see a clear linear trend (but overall there is an upward trend), but we do see a strong seasonality in the price dataset! The residuals appear to be following a pattern too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15f91c",
   "metadata": {},
   "source": [
    "Let's normalize the data - rescale it to the range of 0 to 1, split the data into the train and test datasets. We can not use the cross-validation because the order of the data is important.\n",
    "\n",
    "For each data point starting with the N+1 datapoint the function \"create_dataset\" returns the list of the N previous observations (where N = loop back) as the first output parameter and the current observation as the second output parameter. For instance, for the loopback 3 the function return (x0,x1,x2) as the first output value and (x3) as the last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a984f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "tf.random.set_seed(7)\n",
    "look_back = 15\n",
    "\n",
    "dataset = y_demand['Launches'].values\n",
    "# dataset = y_price['Prices']. values\n",
    "dataset = dataset.astype('float32')\n",
    "reshaped_dataset = dataset.reshape(-1, 1)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(reshaped_dataset)\n",
    "\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size-look_back:len(dataset),:]\n",
    "\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=110, batch_size=1, verbose=2)\n",
    "\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "\n",
    "trainScore = np.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = np.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back):len(dataset), :] = testPredict\n",
    "\n",
    "inbetween = np.empty_like(dataset)\n",
    "inbetween[:, :] = np.nan\n",
    "inbetween[len(trainPredict)+(look_back)-1:len(trainPredict)+(look_back)+1, :] = [trainPredict[-1], testPredict[0]]\n",
    "\n",
    "plt.plot(scaler.inverse_transform(dataset), label='Actual')\n",
    "plt.plot(trainPredictPlot, label='Train set')\n",
    "plt.plot(testPredictPlot, label='Test set')\n",
    "plt.plot(inbetween, linestyle='dashed', color = 'blue', alpha=0.5)\n",
    "plt.legend(loc='upper right')\n",
    "plt.gca().set(title='LSTM user demand prediction', xlabel='Months', ylabel='Launches')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304e0517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def get_data_times(number, dates):\n",
    "    d = dates[len(dates)-1]\n",
    "    list_dates = [d + relativedelta(months=+month_num) for month_num in range(1,number+1)]\n",
    "    return np.append([date for date in dates], list_dates)\n",
    "\n",
    "def make_predictions(number, look_back, dataset):\n",
    "    actuals = []\n",
    "    last_observations = dataset[-look_back:]\n",
    "    last_observations = last_observations.reshape(1, -1)\n",
    "    last_observations = np.reshape(last_observations, (last_observations.shape[0], 1, last_observations.shape[1]))\n",
    "    for i in range(number):\n",
    "        new_predict = model.predict(last_observations)\n",
    "        new_actual = scaler.inverse_transform(new_predict)\n",
    "        actuals.append(new_actual.ravel()[0])\n",
    "        last_observations = np.append(last_observations[:,:,1:], new_predict)\n",
    "        last_observations = last_observations.reshape(1, -1)\n",
    "        last_observations = np.reshape(last_observations, (last_observations.shape[0], 1, last_observations.shape[1]))     \n",
    "    dates = get_data_times(number, dates_demand)\n",
    "    return actuals, dates\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79c55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, datetimes = make_predictions(6, look_back, dataset)\n",
    "\n",
    "predictoinsPlot = np.empty_like(dataset[:-1])\n",
    "predictoinsPlot[:, :] = np.nan\n",
    "first_val = scaler.inverse_transform(dataset[-1].reshape(-1, 1))\n",
    "predictoinsPlot = np.append(predictoinsPlot, np.append(first_val, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a3078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot baseline and predictions\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "actuals = np.append(scaler.inverse_transform(dataset), np.zeros(len(datetimes)- len(scaler.inverse_transform(dataset))) * np.nan)\n",
    "plt.plot(datetimes, actuals, label='Actual')\n",
    "\n",
    "trains = np.append(trainPredictPlot, np.zeros(len(datetimes)- len(trainPredictPlot)) * np.nan)\n",
    "plt.plot(datetimes, trains, label='Train set: RMSE = ' + str(round(trainScore, 2)))\n",
    "\n",
    "tests = np.append(testPredictPlot, np.zeros(len(datetimes)- len(testPredictPlot)) * np.nan)\n",
    "plt.plot(datetimes, tests, label='Test set: RMSE = ' + str(round(testScore, 2)))\n",
    "\n",
    "inbetweens = np.append(inbetween, np.zeros(len(datetimes)- len(inbetween)) * np.nan)\n",
    "plt.plot(datetimes, inbetweens, linestyle='dashed', color = 'blue', alpha=0.5)\n",
    "\n",
    "plt.plot(datetimes, predictoinsPlot, color = 'blue', linewidth=3, label='Predictions')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.gca().set(title='LSTM user demand prediction', xlabel='Months', ylabel='Launches')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34439787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

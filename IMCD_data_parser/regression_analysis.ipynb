{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57a1988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "def get_config():\n",
    "    with open(\"config.yaml\", 'r') as stream:\n",
    "        config = yaml.safe_load(stream)\n",
    "    return config\n",
    "\n",
    "config = get_config()\n",
    "dir_to = config['directory_to']\n",
    "file_name_to = config['cleared_data_to']\n",
    "cleared_data = pd.read_csv(os.path.join(dir_to, file_name_to))\n",
    "cleared_data.head()\n",
    "\n",
    "def get_data_info(arr, show=False, threshold=20):\n",
    "    for ar in arr:\n",
    "        s = set(ar)\n",
    "        if show:\n",
    "            if len(s) < threshold:\n",
    "                print(len(s), s)\n",
    "            else:\n",
    "                print(len(s))\n",
    "        else:\n",
    "            print(len(s))\n",
    "\n",
    "print(cleared_data.shape)\n",
    "\n",
    "# possible columns for regression analysis\n",
    "dates = cleared_data['Event Date'].tolist()  #38\n",
    "categories = cleared_data['Sub-Category'].tolist()  #4\n",
    "regions = cleared_data['Region'].tolist() #2\n",
    "pos_category = cleared_data['Positioning Category'].tolist()   #336, nan category included! - according to stats files\n",
    "pos_sub_category = cleared_data['Positioning Sub-Category'].tolist()  #7785, nan category included!\n",
    "pack_material = cleared_data['Packaging Material'].tolist()  #23, nan category included!\n",
    "shelving_details = cleared_data['Shelving Details'].tolist() #3\n",
    "price_details = cleared_data['Euro Price/Kg'].tolist() #8539 unique, obviously not a category\n",
    "\n",
    "get_data_info([dates, categories, regions, pos_category, pos_sub_category, pack_material, shelving_details, price_details], show=True, threshold=24)\n",
    "# clear the nan values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3db97b",
   "metadata": {},
   "source": [
    "We should not forget to check the amount of non-nan values in the columns considered the undependant variables.\n",
    "We can use the euro price as an undependant variable for regression analysis but there is significant amount of empty values in columns concerning the price values. \n",
    "\n",
    "We can eliminate empty values and predict the future prices or how much the end client is willing to pay for the products on the basis of non empty values.\n",
    "\n",
    "We should ask about the types of plastic in the Packaging Material column. We could use this info as the independant variable.\n",
    "\n",
    "We should ask about the Event 6 types - 'Reformulation', 'New Product', 'Import', 'New Package', 'Shelf Snapshots', 'Shelf SnapShots'. Should we predict the most popular categories according to all of the event types or only according to new launches?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e89ec",
   "metadata": {},
   "source": [
    "Problems:\n",
    "1) Packweight column is parsed wrong. There are no such packweights as 105 kg. In original data the value constitutes 0.105 kg. Check for the entry with 'Product Id' == 8533553 (the first file provided by IMCD)\n",
    "\n",
    "2) Values in columns of the string type are not considered empty even if they are. We get wrong statistics. - FIXED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bd3abe",
   "metadata": {},
   "source": [
    "Let's start the regression analysis. Let Sub-Category be the dependant variable, Event Date and Region - undependant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4734924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_data = cleared_data[['Event Date', 'Sub-Category', 'Region']].dropna(how='any')\n",
    "print(products_data.shape)\n",
    "products_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f585993f",
   "metadata": {},
   "source": [
    "Let's convert the date column to the correct datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ad3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import calendar\n",
    "\n",
    "def month_num(m):\n",
    "    if len(m) == 1:\n",
    "        return '0'+m\n",
    "    return m\n",
    "\n",
    "months = {month: month_num(str(index)) for index, month in enumerate(calendar.month_abbr) if month}\n",
    "\n",
    "def convert_to_date(x):\n",
    "    month, year = x.split()\n",
    "    return '01-'+months[month] +'-'+ year\n",
    "\n",
    "products_data['Event Date'] = products_data['Event Date'].map(convert_to_date)\n",
    "products_data['Event Date'] = pd.to_datetime(products_data['Event Date'], format=\"%d-%m-%Y\")\n",
    "products_data['Event Date'] = products_data['Event Date'].map(datetime.datetime.toordinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36acf112",
   "metadata": {},
   "source": [
    "Let's represent Sub-Category and Region categories as numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe9a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = {'West Europe':0, 'East Europe':1}\n",
    "categories = {'Savory Biscuits/Crackers':0, 'Sweet Biscuits/Cookies':1, 'Cakes - Pastries & Sweet Goods':2, 'Bread & Bread Products':3}\n",
    "\n",
    "def categorize_regions(region):\n",
    "    return regions[region]\n",
    "\n",
    "def categorize_product_category(category):\n",
    "    return categories[category]\n",
    "    \n",
    "products_data['Sub-Category'] = products_data['Sub-Category'].map(categorize_product_category)\n",
    "# products_data['Region'] = products_data['Region'].map(categorize_regions)\n",
    "products_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf2f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, CategoricalColorMapper\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.resources import INLINE\n",
    "output_notebook(INLINE)\n",
    "\n",
    "source = ColumnDataSource(\n",
    "        data = {'Event Date': products_data['Event Date'], 'Sub-Category': products_data['Sub-Category'], 'Region': products_data['Region']}\n",
    "    )\n",
    "\n",
    "palette=['red', 'blue']\n",
    "mapper = CategoricalColorMapper(factors=['West Europe', 'East Europe'], palette=palette)\n",
    "pl = figure(width=900, height=500, title='The relation between the event date and the product category', x_axis_label='Event date', y_axis_label='Sub-Category')\n",
    "sc = pl.scatter('Event Date', 'Sub-Category', source=source, size=9, alpha = 0.05, color={'field': 'Region', 'transform': mapper}, legend_field='Region')\n",
    "pl.add_layout(pl.legend[0], 'right')\n",
    "\n",
    "show(pl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9999e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import BasicTicker, ColorBar, LinearColorMapper, PrintfTickFormatter, FixedTicker\n",
    "\n",
    "products_data_west_europe = products_data[products_data['Region'] == 'West Europe']\n",
    "\n",
    "source_west_europe = ColumnDataSource(\n",
    "        data = {'Event Date': products_data_west_europe['Event Date'], 'Sub-Category': products_data_west_europe['Sub-Category'], 'Region': products_data_west_europe['Region']}\n",
    "    )\n",
    "\n",
    "p = figure(title=\"The relation between the event date and the product categories in west europe\",\n",
    "           x_axis_location=\"above\", width=800, height=400,\n",
    "           toolbar_location='below',\n",
    "           x_axis_label='Event Date', y_axis_label='Sub-Category')\n",
    "\n",
    "p.rect(x=\"Event Date\", y=\"Sub-Category\", width=12, height=1,\n",
    "       source=source_west_europe,\n",
    "       fill_color={'field': 'Region', 'transform': mapper},\n",
    "       line_color='black', alpha=0.02)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58ff30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_data_east_europe = products_data[products_data['Region'] == 'East Europe']\n",
    "\n",
    "source_east_europe = ColumnDataSource(\n",
    "        data = {'Event Date': products_data_east_europe['Event Date'], 'Sub-Category': products_data_east_europe['Sub-Category'], 'Region': products_data_east_europe['Region']}\n",
    "    )\n",
    "\n",
    "p = figure(title=\"The relation between the event date and the product categories in east europe\",\n",
    "           x_axis_location=\"above\", width=800, height=400,\n",
    "           toolbar_location='below',\n",
    "           x_axis_label='Event Date', y_axis_label='Sub-Category')\n",
    "\n",
    "p.rect(x=\"Event Date\", y=\"Sub-Category\", width=12, height=1,\n",
    "       source=source_east_europe,\n",
    "       fill_color={'field': 'Region', 'transform': mapper},\n",
    "       line_color='black', alpha=0.02)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41423368",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_data = cleared_data[['Event Date', 'Euro Price/Kg', 'Sub-Category']].dropna(how='any')\n",
    "print(products_data.shape)\n",
    "\n",
    "products_data['Event Date'] = products_data['Event Date'].map(convert_to_date)\n",
    "products_data['Event Date'] = pd.to_datetime(products_data['Event Date'], format=\"%d-%m-%Y\")\n",
    "products_data['Event Date'] = products_data['Event Date'].map(datetime.datetime.toordinal)\n",
    "products_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4fb96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bread = products_data[products_data['Sub-Category'] == 'Bread & Bread Products']\n",
    "\n",
    "\n",
    "source = ColumnDataSource(\n",
    "        data = {'Event Date': bread['Event Date'], 'Euro Price/Kg': bread['Euro Price/Kg'], 'Sub-Category': bread['Sub-Category']}\n",
    "    )\n",
    "\n",
    "palette=['red', 'blue', 'green', 'pink']\n",
    "mapper = CategoricalColorMapper(factors=['Savory Biscuits/Crackers', 'Sweet Biscuits/Cookies', 'Cakes - Pastries & Sweet Goods', 'Bread & Bread Products'], palette=palette)\n",
    "pl = figure(width=900, height=500, title='The relation between the event date and the pricing', x_axis_label='Event date', y_axis_label='Euro Price/Kg')\n",
    "sc = pl.scatter('Event Date', 'Euro Price/Kg', source=source, size=9, alpha = 0.3, color={'field': 'Sub-Category', 'transform': mapper}, legend_field='Sub-Category')\n",
    "pl.add_layout(pl.legend[0], 'right')\n",
    "\n",
    "show(pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e172eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_data = pd.read_csv(os.path.join(dir_to, file_name_to), parse_dates=['Event Date'])\n",
    "print('unique event types:', cleared_data['Event'].unique())\n",
    "cleared_data = cleared_data[['Event Date', 'Sub-Category', 'Event', 'Region']].dropna(how='any')\n",
    "cleared_data.head()\n",
    "cleared_data['Event Date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3561b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_time_series_analysis(cleared_data, event_type='all', category='all', region='all', add_missing_values=True):\n",
    "    if event_type != 'all':\n",
    "        cleared_data  = cleared_data[cleared_data['Event'] == event_type]\n",
    "    if category != 'all':\n",
    "        cleared_data  = cleared_data[cleared_data['Sub-Category'] == category]\n",
    "    if region != 'all':\n",
    "        cleared_data = cleared_data[cleared_data['Region'] == region]\n",
    "    cleared_data = cleared_data.groupby(['Event Date']).count() \n",
    "    # add missing dates, MS - calendar month begin\n",
    "    if add_missing_values:\n",
    "        cleared_data = cleared_data.asfreq(freq='MS')\n",
    "        # should interpolate? (polynomial or time) or should fill with zeroes?\n",
    "#         cleared_data = cleared_data.interpolate(method='time', order=2).reset_index()\n",
    "        cleared_data = cleared_data.fillna(0).reset_index()\n",
    "    else:\n",
    "        cleared_data = cleared_data.reset_index() \n",
    "    cleared_data = cleared_data[['Event Date', 'Sub-Category']]\n",
    "    # consider only 2022 year, values in 2023 seem to be unreliable\n",
    "    return cleared_data[cleared_data['Event Date'] < '2023-01-01']\n",
    "#     return cleared_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce1501a",
   "metadata": {},
   "source": [
    "If we take a look at the date frames we can see that some months are missing for certain categories. We are going to add them for each category and region separately and interpolate. Are missing: 3 months in 2020, 8 months in 2021 for the cakes category (period from 10.2020 to 08.2021 is missing)\n",
    "Interpolation doesn't work well because a huge period of time is missing. Does it mean that there were no new launches? Should we interpolate it or fill with 0? We could use forward-fill, backward-fill, forecast it using past observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754ee054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_missing_months(cleared_data, event_type, category, region):\n",
    "    data = get_data_for_time_series_analysis(cleared_data, event_type=event_type, category=category, region = region, add_missing_values = False)\n",
    "    print(event_type+':'+category+':'+region+':', len(data), '/', len(cleared_data['Event Date'].unique())-2)\n",
    "    \n",
    "get_number_of_missing_months(cleared_data, 'New Product', 'Cakes - Pastries & Sweet Goods', 'West Europe')\n",
    "get_number_of_missing_months(cleared_data, 'New Product', 'Cakes - Pastries & Sweet Goods', 'East Europe')\n",
    "get_number_of_missing_months(cleared_data, 'New Product', 'Bread & Bread Products', 'West Europe')\n",
    "get_number_of_missing_months(cleared_data, 'New Product', 'Bread & Bread Products', 'East Europe')\n",
    "get_number_of_missing_months(cleared_data, 'New Product', 'Savory Biscuits/Crackers', 'West Europe')\n",
    "get_number_of_missing_months(cleared_data, 'New Product', 'Savory Biscuits/Crackers', 'East Europe')\n",
    "get_number_of_missing_months(cleared_data, 'New Product', 'Sweet Biscuits/Cookies', 'West Europe')\n",
    "get_number_of_missing_months(cleared_data, 'New Product', 'Sweet Biscuits/Cookies', 'East Europe')\n",
    "\n",
    "get_number_of_missing_months(cleared_data, 'New Product', 'Cakes - Pastries & Sweet Goods', 'all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc436ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for any type of event, for all categories of products in east europe\n",
    "data1 = get_data_for_time_series_analysis(cleared_data, event_type='New Product')\n",
    "\n",
    "# data for New Product (new launch) event, for all categories of products in east europe\n",
    "data2 = get_data_for_time_series_analysis(cleared_data, event_type='New Product', region = 'East Europe')\n",
    "\n",
    "# data for any type of event, for certain category of products (Bread & Bread Products) in east europe\n",
    "data3 = get_data_for_time_series_analysis(cleared_data, category='Bread & Bread Products', region = 'East Europe')\n",
    "\n",
    "# data for New Product (new launch) event, for certain category of products (Bread & Bread Products) in east europe\n",
    "data4 = get_data_for_time_series_analysis(cleared_data, event_type='New Product', category='Bread & Bread Products', region = 'East Europe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc0b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Draw Plot\n",
    "def plot_df(x, y, title=\"\", xlabel='Date', ylabel='Value'):\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.plot(x, y, color='tab:red')\n",
    "    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    plt.show()\n",
    "\n",
    "plot_df(x=data1['Event Date'], y=data1['Sub-Category'], title='Product events between 2020 and 2022 years in East Europe')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter  \n",
    "\n",
    "def plot_event_lines_for_all_categories_together(cleared_data, region = 'all'):\n",
    "\n",
    "    data_cakes = get_data_for_time_series_analysis(cleared_data, event_type='New Product', category='Cakes - Pastries & Sweet Goods', region = region)\n",
    "    data_crackers = get_data_for_time_series_analysis(cleared_data, event_type='New Product', category='Savory Biscuits/Crackers', region = region)\n",
    "    data_cookies = get_data_for_time_series_analysis(cleared_data, event_type='New Product', category='Sweet Biscuits/Cookies', region = region)\n",
    "    data_bread = get_data_for_time_series_analysis(cleared_data, event_type='New Product', category='Bread & Bread Products', region = region)\n",
    "\n",
    "    plt.figure(figsize=(16,10))\n",
    "    x_filtered = data_cakes[[\"Sub-Category\"]].apply(savgol_filter,  window_length=15, polyorder=3)\n",
    "    plt.plot(data_cakes['Event Date'], data_cakes['Sub-Category'], label='Cakes', color='red')\n",
    "    plt.plot(data_cakes['Event Date'], x_filtered, color='red', linestyle='dashed')\n",
    "\n",
    "    x_filtered = data_crackers[[\"Sub-Category\"]].apply(savgol_filter,  window_length=15, polyorder=3)\n",
    "    plt.plot(data_crackers['Event Date'], data_crackers['Sub-Category'], label='Savory Biscuits/Crackers', color='green')\n",
    "    plt.plot(data_crackers['Event Date'], x_filtered, color='green', linestyle='dashed')\n",
    "\n",
    "    x_filtered = data_cookies[[\"Sub-Category\"]].apply(savgol_filter,  window_length=15, polyorder=3)\n",
    "    plt.plot(data_cookies['Event Date'], data_cookies['Sub-Category'], label='Sweet Biscuits/Cookies', color='violet')\n",
    "    plt.plot(data_cookies['Event Date'], x_filtered, color='violet', linestyle='dashed')\n",
    "\n",
    "    x_filtered = data_bread[[\"Sub-Category\"]].apply(savgol_filter,  window_length=15, polyorder=3)\n",
    "    plt.plot(data_bread['Event Date'], data_bread['Sub-Category'], label='Bread', color='blue')\n",
    "    plt.plot(data_bread['Event Date'], x_filtered, color='blue', linestyle='dashed')\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    if region != 'all':\n",
    "        title_part = region\n",
    "    else:\n",
    "        title_part = 'Europe'\n",
    "\n",
    "    plt.gca().set(title='Product launches between 2020 and 2022 years in ' + title_part, xlabel='Date', ylabel='Events')\n",
    "    plt.show()\n",
    "    \n",
    "plot_event_lines_for_all_categories_together(cleared_data)\n",
    "plot_event_lines_for_all_categories_together(cleared_data, region = 'West Europe')\n",
    "plot_event_lines_for_all_categories_together(cleared_data, region = 'East Europe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f66b9f7",
   "metadata": {},
   "source": [
    "No trends are represented for any category of the products because there is no increasing or decreasing slope observed in the time series. But there may be seasonality - a distinct repeated pattern observed between regular intervals due to seasonal factors. We will check it below.\n",
    "\n",
    "Another aspect to consider is the cyclic behaviour. It happens when the rise and fall pattern in the series does not happen in fixed calendar-based intervals. For now I would say the behavior of the Sweet Biscuits/Cookies pink line looks wavy with the period of a year and a half. This behaviour is cyclic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843059de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_event_lines_for_all_categories_together(cleared_data, region = 'West Europe')\n",
    "# graph strongly depends on the interpolation method/filling in with zeroes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec83c06",
   "metadata": {},
   "source": [
    "Since its a monthly time series let's see if it follows a certain repetitive pattern every year. Let's plot each year as a separate line in the same plot. This lets us compare the year wise patterns side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73633e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "def plot_seasonal_year_pattern(cleared_data, event_type='New Product', category='all', region = 'all'):\n",
    "\n",
    "    data_bread = get_data_for_time_series_analysis(cleared_data, event_type=event_type, category=category, region = region)\n",
    "    data_bread['year'] = [d.year for d in data_bread['Event Date']]\n",
    "    data_bread['month'] = [d.strftime('%b') for d in data_bread['Event Date']]\n",
    "    years = data_bread['year'].unique()\n",
    "\n",
    "    np.random.seed(100)\n",
    "    mycolors = np.random.choice(list(mpl.colors.XKCD_COLORS.keys()), len(years), replace=False)\n",
    "\n",
    "    plt.figure(figsize=(16,10))\n",
    "    for i, y in enumerate(years):\n",
    "        data = data_bread.loc[data_bread.year==y, :]\n",
    "        x_filtered = data[[\"Sub-Category\"]].apply(savgol_filter,  window_length=11, polyorder=3)\n",
    "        plt.plot('month', 'Sub-Category', data=data, color=mycolors[i], label=y)\n",
    "        plt.text(data_bread.loc[data_bread.year==y, :].shape[0]-.9, data_bread.loc[data_bread.year==y, 'Sub-Category'][-1:].values[0], y, fontsize=12, color=mycolors[i])\n",
    "        plt.plot(data['month'], x_filtered, color=mycolors[i], linestyle='dashed')\n",
    "\n",
    "\n",
    "    # Decoration\n",
    "    plt.gca().set(ylabel='Number of new launches', xlabel='Month')\n",
    "    plt.yticks(fontsize=12, alpha=.7)\n",
    "    plt.title(\"Seasonal Plot of \" + category + \" for 2020, 2021 and 2022 years in \" + region, fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plot_seasonal_year_pattern(cleared_data, event_type='New Product', category='Sweet Biscuits/Cookies', region = 'West Europe')\n",
    "plot_seasonal_year_pattern(cleared_data, event_type='New Product', category='Bread & Bread Products', region = 'West Europe')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae4830",
   "metadata": {},
   "source": [
    "On the plot above we can see that there is certain pattern that repeats itself from year to year - the number of launches decreases (or stabilizes) in the late summer- early fall period and grows back in the end of the year for the Bread & Bread Products category of products. Whereas no seasonality can be seen for the Sweet Biscuits/Cookies category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data_bread = get_data_for_time_series_analysis(cleared_data, event_type='New Product', category='Bread & Bread Products', region = 'West Europe')\n",
    "data_bread['year'] = [d.year for d in data_bread['Event Date']]\n",
    "b=sns.boxplot(x='year', y='Sub-Category', data=data_bread)\n",
    "plt.title('Year-wise Box Plot(The Trend) for Bread & Bread Products in West Europe')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b1a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kde\n",
    "\n",
    "months = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \n",
    "              \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "\n",
    "def plot_monthly_hist(cleared_data, event_type='New Product', category='all', region = 'all', color='green'):\n",
    "    data_bread = get_data_for_time_series_analysis(cleared_data, event_type=event_type, category=category, region = region)\n",
    "    data_bread['month'] = [d.strftime('%b') for d in data_bread['Event Date']]\n",
    "    \n",
    "    data_bread_gb = data_bread[['month', 'Sub-Category']]\n",
    "    data_bread_gb = data_bread_gb.groupby(['month']).sum().reset_index()\n",
    "    data_bread_gb['month'] = pd.Categorical(data_bread_gb['month'], categories=months, ordered=True)\n",
    "    data_bread_gb.sort_values(by='month', inplace=True) \n",
    "    x, y = data_bread_gb['month'], data_bread_gb['Sub-Category']\n",
    "    plt.bar(x, y, edgecolor=\"k\", color=color)\n",
    "    plt.title(\"Distribution of monthly launches of \" + category + \" for 2020, 2021 and 2022 years in \" + region, fontsize=10)\n",
    "    plt.show()\n",
    "\n",
    "# plot_monthly_hist(cleared_data, event_type='New Product', category='Bread & Bread Products', region = 'West Europe', color='blue')\n",
    "# plot_monthly_hist(cleared_data, event_type='New Product', category='Bread & Bread Products', region = 'East Europe', color='pink')\n",
    "\n",
    "plot_monthly_hist(cleared_data, event_type='New Product', category='Savory Biscuits/Crackers', region = 'West Europe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23496dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter  \n",
    "\n",
    "# in West Europe cake info is missing\n",
    "\n",
    "data_cakes = get_data_for_time_series_analysis(cleared_data, event_type='New Product', region = 'East Europe')\n",
    "plt.figure(figsize=(16,10))\n",
    "x_filtered = data_cakes[[\"Sub-Category\"]].apply(savgol_filter,  window_length=15, polyorder=3)\n",
    "plt.plot(data_cakes['Event Date'], data_cakes['Sub-Category'], color='red')\n",
    "plt.plot(data_cakes['Event Date'], x_filtered, color='red', linestyle='dashed')\n",
    "plt.gca().set(title='Product launches between 2020 and 2022 years in East Europe', xlabel='Date', ylabel='Events')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928493f2",
   "metadata": {},
   "source": [
    "Let's establish the stationarity of a series. A stationary series is one where the values of the series is not a function of time. It means that the statistical properties of the series like mean, variance and autocorrelation are constant over time. Autocorrelation of the series is the correlation of the series with its previous values.\n",
    "\n",
    "Following the Augmented Dickey Fuller test (ADH Test) data is stationary if p-value is less than 0.05. \n",
    "\n",
    "We can notice that not all datasets are stationary! But the forecasts for a stationary series are more reliable. And autoregressive forecasting models (one of them we are going to use to predict client demand in products) are essentially linear regression models that utilize the lag(s) of the series itself as predictors.\n",
    "\n",
    "We know that linear regression works best if the predictors (X variables) are not correlated against each other. So, stationarizing the series solves this problem since it removes any persistent autocorrelation, thereby making the predictors(lags of the series) in the forecasting models nearly independent.\n",
    "\n",
    "So before conducting an analysis we have to make the datasets stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f796be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "def is_data_stationary(cleared_data, event_type='New Product', category='all', region = 'all'):\n",
    "    data = get_data_for_time_series_analysis(cleared_data, event_type=event_type, category=category, region = region)\n",
    "    \n",
    "    # ADF Test\n",
    "    result = adfuller(data['Sub-Category'].values, autolag='AIC')\n",
    "    return data, result[1] < 0.05\n",
    "\n",
    "categories['all'] = 5\n",
    "regions['all'] = 2\n",
    "for c in categories:\n",
    "    for r in regions:\n",
    "        _, is_stationary = is_data_stationary(cleared_data, event_type='New Product', category=c, region = r)\n",
    "        print(c, r, is_stationary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b83e78",
   "metadata": {},
   "source": [
    "The Pearson’s correlation coefficient is a number between -1 and 1 that describes a negative or positive correlation respectively. A value of zero indicates no correlation. The Pearson’s correlation coefficient is depicted as the Y axis.\n",
    "\n",
    "We can calculate the correlation for time series observations with observations with previous time steps, called lags. Because the correlation of the time series observations is calculated with values of the same series at previous times, this is called a serial correlation, or an autocorrelation.\n",
    "\n",
    "The plot below shows the lag value along the x-axis and the correlation on the y-axis between -1 and 1. We expect the ACF - AutoCorrelation Function - time series to be strong to a lag of k and the inertia of that relationship would carry on to subsequent lag values, trailing off at some point as the effect was weakened.\n",
    "\n",
    "Confidence intervals are drawn as a cone. By default, this is set to a 95% confidence interval, suggesting that correlation values outside of this code are very likely a correlation and not a statistical fluke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54828bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "def plot_lag_correlation(cleared_data, event_type='New Product', category='all', region = 'all'):\n",
    "    data = get_data_for_time_series_analysis(cleared_data, event_type=event_type, category=category, region = region)\n",
    "    plot_acf(data['Sub-Category'])\n",
    "    plot_pacf(data['Sub-Category'])\n",
    "    \n",
    "# is stationary\n",
    "plot_lag_correlation(cleared_data, event_type='New Product', category='Savory Biscuits/Crackers', region = 'West Europe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dcf09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is not stationary\n",
    "plot_lag_correlation(cleared_data, event_type='New Product', category='Bread & Bread Products', region = 'East Europe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd45de2",
   "metadata": {},
   "source": [
    "According to the plots it is likely a white noise because there is no correlation between current observations and lags (past observations). It makes it impossible to use the autogressive model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2353a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "\n",
    "def plot_lag_correlation(cleared_data, event_type='New Product', category='all', region = 'all'):\n",
    "    data = get_data_for_time_series_analysis(cleared_data, event_type=event_type, category=category, region = region)\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10,3), sharex=True, sharey=True, dpi=100)\n",
    "    for i, ax in enumerate(axes.flatten()[:4]):\n",
    "        lag_plot(data['Sub-Category'], lag=i+1, ax=ax, c='firebrick')\n",
    "        ax.set_title('Lag ' + str(i+1))\n",
    "\n",
    "    fig.suptitle('Lag Plots\\n(Points get wide and scattered with increasing lag -> lesser correlation)\\n', y=1.15)    \n",
    "    plt.show()\n",
    "plot_lag_correlation(cleared_data, event_type='New Product', category='Savory Biscuits/Crackers', region = 'West Europe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def get_sample_entropy_stats(cleared_data, event_type='New Product', category='all', region = 'all'):\n",
    "    data = get_data_for_time_series_analysis(cleared_data, event_type=event_type, category=category, region = region)\n",
    "    return entropy(data['Sub-Category'])\n",
    "\n",
    "print(get_sample_entropy_stats(cleared_data, event_type='New Product', category='Savory Biscuits/Crackers', region = 'West Europe'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7257e",
   "metadata": {},
   "source": [
    "Let's conduct Granger causality test to see if one time series will be useful to forecast another. The null hypothesis is that the values in the second column can not predict the values in the first column. All p-values that we get are larger than 0.05 so we can not reject the null hypothesis. So it's impossible to say that we can predict the user demand using the month info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae3da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "def is_data_predictable_granger_causality(cleared_data, event_type='New Product', category='all', region = 'all'):\n",
    "    data = get_data_for_time_series_analysis(cleared_data, event_type=event_type, category=category, region = region)\n",
    "    data['month'] = data['Event Date'].dt.month\n",
    "\n",
    "    return grangercausalitytests(data[['Sub-Category', 'month']], maxlag=2)\n",
    "\n",
    "print(is_data_predictable_granger_causality(cleared_data, event_type='New Product', category='Savory Biscuits/Crackers', region = 'West Europe'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4593b11",
   "metadata": {},
   "source": [
    "Now we want to make a forecast for the client demand using ARIMA model. For this purpose we need to define several parameters:\n",
    "\n",
    "p - the order of the AR term. It refers to the number of lags of Y to be used as predictors;\n",
    "\n",
    "q - the order of the MA term. It refers to the number of lagged forecast errors that should go into the ARIMA Model;\n",
    "\n",
    "d - the number of differencing required to make the time series stationary.\n",
    "\n",
    "Some of the datasets are stationary, for them the d parameter equals 0. Other datasets we have to make stationary. There are several ways to make them stationary:\n",
    "\n",
    "Differencing the Series (once or more)\n",
    "\n",
    "Take the log of the series\n",
    "\n",
    "Take the nth root of the series\n",
    "\n",
    "Combination of the above\n",
    "\n",
    "We are going to use differencing technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b9ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's work on a data which is not stationary: Bread & Bread Products in east Europe\n",
    "plt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})\n",
    "\n",
    "def adf_is_stationary(values):\n",
    "    result = adfuller(values, autolag='AIC')\n",
    "    return result[1] < 0.05\n",
    "\n",
    "def define_differencing_order_plot(cleared_data, event_type='New Product', category='all', region = 'all'):\n",
    "    data = get_data_for_time_series_analysis(cleared_data, event_type=event_type, category=category, region = region)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, sharex=True)\n",
    "    \n",
    "    in_data = data['Sub-Category']\n",
    "    axes[0, 0].plot(in_data)\n",
    "    axes[0, 0].set_title('Original Series:'+str(adf_is_stationary(in_data.dropna())))\n",
    "    plot_acf(in_data, ax=axes[0, 1])\n",
    "\n",
    "    # 1st Differencing\n",
    "    in_data = in_data.diff()\n",
    "    axes[1, 0].plot(in_data)\n",
    "    axes[1, 0].set_title('1st Order Differencing:'+str(adf_is_stationary(in_data.dropna())))\n",
    "    plot_acf(in_data.dropna(), ax=axes[1, 1])\n",
    "\n",
    "    # 2nd Differencing\n",
    "    in_data = in_data.diff()\n",
    "    axes[2, 0].plot(in_data) \n",
    "    axes[2, 0].set_title('2nd Order Differencing:'+str(adf_is_stationary(in_data.dropna())))\n",
    "    plot_acf(in_data.dropna(), ax=axes[2, 1])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "define_differencing_order_plot(cleared_data, event_type='New Product', category='Bread & Bread Products', region = 'East Europe')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e37b50d",
   "metadata": {},
   "source": [
    "For the above series, the time series reaches stationarity with one order of differencing. On looking at the autocorrelation plot for the 2nd differencing the lag goes into the far negative zone fairly quick, which indicates, the series might have been over differenced.\n",
    "\n",
    "So, for Bread & Bread Products in East Europe we are going to choose the order of differencing as 1 (d parameter in ARIMA model). We will automatically define the order of differencing for other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2260596",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_key(event_type, category, region):\n",
    "    return  f'{event_type}:{category}:{region}'\n",
    "\n",
    "def make_data_stationary(data):\n",
    "    in_data = data['Sub-Category'].dropna()\n",
    "    origin = in_data\n",
    "    diff_order = 0\n",
    "    while not adf_is_stationary(in_data):\n",
    "        in_data = in_data.diff().dropna()\n",
    "        diff_order += 1\n",
    "    return in_data, diff_order, origin\n",
    "        \n",
    "        \n",
    "datasets = {}\n",
    "for c in categories:\n",
    "    for r in regions:\n",
    "        data = get_data_for_time_series_analysis(cleared_data, event_type='New Product', category=c, region = r)\n",
    "        stationary_data, diff_order, origin_data = make_data_stationary(data)\n",
    "        datasets[get_key('New Product', c, r)] = (stationary_data, diff_order, origin_data)\n",
    "\n",
    "for k, v in datasets.items():\n",
    "    print(f'Order of differencing for {k} is {v[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c198b56c",
   "metadata": {},
   "source": [
    "AR or MA terms are needed to correct any autocorrelation that remains in the differenced series. To determine the AR and MA terms we have to plot the acf (autocorrelation function) and pacf (partial autocorrelation function).\n",
    "\n",
    "The autocorrelation at lag 1 \"propagates\" to lag 2 and presumably to higher-order lags. But a partial autocorrelation is the amount of correlation between a variable and a lag of itself that is not explained by correlations at all lower-order-lags aka their mutual correlations.\n",
    "\n",
    "The rule of thumb is if the partial autocorrelation is significant at lag k and not significant at any higher order lags, i.e. if the PACF \"cuts off\" at lag k, then this suggests that you should try fitting an autoregressive model of order k (which is parameter p in the ARIBA model). For instance, if the PACF plot has a significant spike only at lag 1 it means that all the higher-order autocorrelations are effectively explained by the lag-1 autocorrelation and the p parameter in ARIBA model should equal to 1 (on the plots below we don't count the first point - the lag-0 - which obviosly correlates to itself with the most possible coefficient - 1).\n",
    "\n",
    "For example, for Bread & Bread Products in West Europe we can see that the PACF lag 1 is quite significant since it's above the significance line. Lag 2 turns out to be not significant because it falls below the significance limit (blue region). So the p parameter for ARIBA model would be 1.\n",
    "\n",
    "Just like how we looked at the PACF plot for the number of AR terms, we can look at the ACF plot for the number of MA terms. The ACF tells how many MA terms are required to remove any autocorrelation in the stationarized series. For example, for Sweet Biscuits/Cookies in West Europe the ACF cuts off the lags after lag-2. So the q parameter (MA parameter) for the ARIBA model would be 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57e6a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in datasets.items():\n",
    "    fig, axes = plt.subplots(1, 3, sharex=True)\n",
    "    axes[0].plot(v[0])\n",
    "    axes[0].set_title(k, fontsize=6)\n",
    "    plot_acf(v[0], ax=axes[1])\n",
    "    plot_pacf(v[0], ax=axes[2])\n",
    "\n",
    "# according to the ACF and PACF plots define the p and q parameters for the ARIBA model:  \n",
    "# [p(look at PCAF), q(look at CAF)]      \n",
    "params = [[0, 0], [3, 0], [1, 1], [1, 2], [0, 0], [1, 1], [0, 0], [0, 0], [0, 0], [1, 1], [0, 0], [1, 1], [0, 0], [2, 1], [0, 0]]\n",
    "i = 0\n",
    "print(\"p, q parameters for ARIBA model:\")\n",
    "datasets_params = {}\n",
    "for k,v in datasets.items():\n",
    "    datasets_params[k]=params[i]\n",
    "    print(k, params[i], '\\n')\n",
    "    i+=1  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133cc67",
   "metadata": {},
   "source": [
    "The data is stationarized and parameters for ARIBA model are defines. Let's fit the ARIMA model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb4e7e2",
   "metadata": {},
   "source": [
    "The P Values of the AR1 and MA1 terms for New Product:Savory Biscuits/Crackers:West Europe are highly significant (<< 0.05). We also get a line plot of the residual errors, suggesting that there may still be some trend information not captured by the model. And we get a density plot of the residual error values. The distribution of the residual errors shows that indeed there is no bias in the prediction (a zero mean in the residuals) which indicates a good model (really?)\n",
    "\n",
    "Ideally we would perform this analysis on just the training dataset when developing a predictive model.\n",
    "\n",
    "Also to define the ideal parameters (p, q) we should try to increase/decrease p and q to see which model gives least AIC and also look for a chart that gives closer actuals and forecasts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb2deba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we already stationarized the data so we should always pass the d param as 0 and stationarized data \n",
    "# or unstationarized data and the actual d parameter\n",
    "\n",
    "from pandas import DataFrame\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "def build_model(key):    \n",
    "    data_stationarized, d, origin_data = datasets[key]\n",
    "    p, q = datasets_params[key]\n",
    "\n",
    "    model = ARIMA(origin_data, order=(p, d, q))\n",
    "\n",
    "    model_fit = model.fit()\n",
    "    print(model_fit.summary())\n",
    "\n",
    "    # residuals + errors density\n",
    "    residuals = DataFrame(model_fit.resid)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10,3))\n",
    "    residuals.plot(title=\"Residuals\", ax=ax[0], legend=False)\n",
    "    residuals.plot(kind='kde', title='Density', ax=ax[1], legend=False)\n",
    "    plt.show()\n",
    "    \n",
    "    # summary stats of residuals\n",
    "    print(residuals.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9802a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from [3, 1] optimized to [3, 0] because the p value of the MA1 term was highly insignificant (>> 0.05) which \n",
    "# made other coefficient significant (<< 0.05) but did not lower the AIC (it increased from 192 to 196)\n",
    "build_model('New Product:Savory Biscuits/Crackers:East Europe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7340dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model('New Product:Savory Biscuits/Crackers:West Europe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a485c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters need to be refines \n",
    "for k,v in datasets.items():\n",
    "    build_model(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc7c95",
   "metadata": {},
   "source": [
    "Let's experiment with the 'New Product:Savory Biscuits/Crackers:West Europe' dataset. We can evaluate an ARIMA model using a walk-forward (the rolling forecast) validation or Out-of-Time Cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate an ARIMA model using a walk-forward validation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def plot_walk_forward_validation(key, p=-1, d=-1, q=-1, points_to_predict=5, train_recentage=0.66):\n",
    "    if train_recentage > 1:\n",
    "        raise Exepception()\n",
    "    data_stationarized, d_estimated, origin_data = datasets[key]\n",
    "    p_estimated, q_estimated = datasets_params[key]\n",
    "    if p!=-1:\n",
    "        p_estimated = p\n",
    "    if q!=-1:\n",
    "        q_estimated = q\n",
    "    if d!=-1:\n",
    "        d_estimated = d\n",
    "\n",
    "    X = origin_data.values\n",
    "    size = int(len(X) * train_recentage)\n",
    "    train, test = X[0:size], X[size:len(X)]\n",
    "    history = [x for x in train]\n",
    "    predictions, predictions_new = list(), list()\n",
    "    print(p_estimated, d_estimated, q_estimated)\n",
    "\n",
    "    # walk-forward validation\n",
    "    for t in range(len(test)):\n",
    "        model = ARIMA(history, order=(p_estimated, d_estimated, q_estimated))\n",
    "        model_fit = model.fit()\n",
    "        output = model_fit.forecast()\n",
    "        predictions.append(output[0])\n",
    "        predictions_new.append(output[0])\n",
    "        obs = test[t]\n",
    "        history.append(obs)\n",
    "        \n",
    "    predictions_points = model_fit.predict(start=len(X), end=len(X)+points_to_predict)\n",
    "    predictions_new = np.append(predictions_new, predictions_points)\n",
    "\n",
    "\n",
    "    corr = np.corrcoef(predictions, test)[0,1]\n",
    "    print('correlation:', corr)\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    pyplot.plot(X, label='actual')\n",
    "    pyplot.plot(np.append(train, predictions_new), label='forecast_new')\n",
    "    pyplot.plot(np.append(train, predictions), label='forecast')\n",
    "    pyplot.plot(train, label='training')\n",
    "\n",
    "    pyplot.gca().set(title='Walk Forward Validation:'+key)\n",
    "    pyplot.legend(loc='upper right')\n",
    "    pyplot.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549fe90b",
   "metadata": {},
   "source": [
    "The prediction plot looks shifted in relation to original values. This is a standard property of one-step ahead prediction or forecasting (with d=1 order of differencing).\n",
    "\n",
    "The information used for the forecast is the history up to and including the previous period. A peak, for example, at a period will affect the forecast for the next period, but cannot influence the forecast for the peak period. This makes the forecasts appear shifted in the plot.\n",
    "\n",
    "A two-step ahead forecast would give the impression of a shift by two periods. Without adding the exogenous regressors or the order of the seasonal component we can not do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04cb2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_walk_forward_validation('New Product:Bread & Bread Products:West Europe', 3,0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bf1406",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in datasets.items():\n",
    "    plot_walk_forward_validation(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1484e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_of_time_cross_validation_plot(key, p=-1, d=-1, q=-1, points_to_predict=5, train_recentage=0.66):\n",
    "    data_stationarized, d_estimated, origin_data = datasets[key]\n",
    "    p_estimated, q_estimated = datasets_params[key]\n",
    "    if p!=-1:\n",
    "        p_estimated = p\n",
    "    if q!=-1:\n",
    "        q_estimated = q\n",
    "    if d!=-1:\n",
    "        d_estimated = d\n",
    "\n",
    "    X = origin_data.values\n",
    "    size = int(len(X) * train_recentage)\n",
    "    train, test = X[0:size], X[size:len(X)]\n",
    "    \n",
    "    print(p_estimated, d_estimated, q_estimated)\n",
    "    model = ARIMA(train, order=(p_estimated, d_estimated, q_estimated))\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    predictions = model_fit.predict(start=size, end=len(X)+points_to_predict)\n",
    "    corr = np.corrcoef(predictions[:len(test)], test)[0,1]\n",
    "    print('correlation:', corr)\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(X, label='actual')\n",
    "    plt.plot(np.append(train, predictions), label='forecast_new')\n",
    "    plt.plot(np.append(train, predictions)[:len(X)], label='forecast')\n",
    "    plt.plot(train, label='training')\n",
    "    plt.title('Out of Time Cross Validation:'+ key)\n",
    "    plt.legend(loc='upper right', fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37768a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_time_cross_validation_plot('New Product:Bread & Bread Products:West Europe', 3,0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3f1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in datasets.items():\n",
    "    out_of_time_cross_validation_plot(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe6ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_walk_forward_validation('New Product:Sweet Biscuits/Cookies:East Europe', 2, 0, 2)\n",
    "out_of_time_cross_validation_plot('New Product:Sweet Biscuits/Cookies:East Europe', 2, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72088c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_walk_forward_validation('New Product:Bread & Bread Products:West Europe', 3,0,3)\n",
    "out_of_time_cross_validation_plot('New Product:Bread & Bread Products:West Europe', 3,0,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74e8e6f",
   "metadata": {},
   "source": [
    "Let's choose the best p, d, q parameters for ARIMA model automatically. Best model means lowest AIC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c07cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "\n",
    "def choose_best_params_automatically(key):\n",
    "    print(key)\n",
    "    _, _, origin_data = datasets[key]\n",
    "    model = pm.auto_arima(origin_data.values, start_p=0, start_q=0,\n",
    "                          test='adf',       # use adftest to find optimal 'd'\n",
    "                          max_p=5, max_q=5, # maximum p and q\n",
    "                          m=1,              # frequency of series\n",
    "                          d=None,           # let model determine 'd'\n",
    "                          seasonal=False,   # No Seasonality\n",
    "                          start_P=0, \n",
    "                          D=0, \n",
    "                          trace=True,\n",
    "                          error_action='ignore',  \n",
    "                          suppress_warnings=True, \n",
    "                          stepwise=True)\n",
    "\n",
    "#     print(model.summary())\n",
    "choose_best_params_automatically('New Product:Sweet Biscuits/Cookies:East Europe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_walk_forward_validation('New Product:Sweet Biscuits/Cookies:East Europe', 1, 0, 0)\n",
    "out_of_time_cross_validation_plot('New Product:Sweet Biscuits/Cookies:East Europe', 1, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_validation_for_all_datasets():\n",
    "    for k,v in datasets.items():\n",
    "        choose_best_params_automatically(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c867adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_forward_validation_for_all_datasets()\n",
    "params = {'New Product:Savory Biscuits/Crackers:West Europe': [0,1,1],\n",
    "         'New Product:Savory Biscuits/Crackers:East Europe': [3,2,0],\n",
    "         'New Product:Savory Biscuits/Crackers:all': [0,1,1],\n",
    "         'New Product:Sweet Biscuits/Cookies:West Europe': [3,2,0],\n",
    "         'New Product:Sweet Biscuits/Cookies:East Europe': [1,0,0],\n",
    "         'New Product:Sweet Biscuits/Cookies:all': [1,2,0],\n",
    "         'New Product:Cakes - Pastries & Sweet Goods:West Europe': [0,2,1],\n",
    "         'New Product:Cakes - Pastries & Sweet Goods:East Europe': [0,1,0],\n",
    "         'New Product:Cakes - Pastries & Sweet Goods:all': [3,2,0],\n",
    "         'New Product:Bread & Bread Products:West Europe': [0,1,3],\n",
    "         'New Product:Bread & Bread Products:East Europe': [1,2,0],\n",
    "         'New Product:Bread & Bread Products:all': [1,1,1],\n",
    "         'New Product:all:West Europe': [1,2,0],\n",
    "         'New Product:all:East Europe': [3,2,0],\n",
    "         'New Product:all:all': [1,2,0]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b91817",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, pars in params.items():\n",
    "    plot_walk_forward_validation(key, pars[0], pars[1], pars[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d6a075",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, pars in params.items():\n",
    "    out_of_time_cross_validation_plot(key, pars[0], pars[1], pars[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e63f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c86732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, read_excel\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "\n",
    "class XML_Stats():\n",
    "    _counter = 0\n",
    "\n",
    "    # file_paths contains one elem - original location or several elements - blending of several data sources\n",
    "    def __init__(self, file_paths, df = None):\n",
    "        XML_Stats._counter += 1\n",
    "        self.id = XML_Stats._counter\n",
    "        \n",
    "        self.file_paths = file_paths\n",
    "        if len(file_paths) == 1:\n",
    "            self.df = self.read_data()\n",
    "        else:\n",
    "            if df is None:\n",
    "                raise Exception('dataframe of XML_Stats can not be None!')\n",
    "            self.df = df\n",
    "        \n",
    "    def read_data(self):\n",
    "        return pd.read_excel(self.file_paths[0])\n",
    "    \n",
    "    def get_columns_amount(self):\n",
    "        return len(self.get_columns())\n",
    "    \n",
    "    def get_columns(self):\n",
    "        return self.df.columns.values.tolist()\n",
    "        \n",
    "    def get_lines_amount(self):\n",
    "        return dict(self.df.count())['S.No.']\n",
    "        \n",
    "    def write_to_csv(self, dir_to, file_name='cleared_merged_data.csv'):\n",
    "        file_path = os.path.join(dir_to, file_name)\n",
    "        self.df.to_csv(file_path, encoding='utf-8', index=False)\n",
    "        \n",
    "    def get_dataframe(self):\n",
    "        return self.df\n",
    "    \n",
    "    def are_mergable(self, xmlstats_obj):\n",
    "        col1 = self.get_columns()\n",
    "        col2 = xmlstats_obj.get_columns()\n",
    "        for c in col1:\n",
    "            if c not in col2 or self.df[c].dtype != xmlstats_obj.df[c].dtype:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def get_possible_merging_problems(self, xmlstats_obj):\n",
    "        str_res = self.file_paths[0] + ' +\\n' + xmlstats_obj.file_paths[0] + '\\n'\n",
    "        col1 = self.get_columns()\n",
    "        col2 = xmlstats_obj.get_columns()\n",
    "        errors = 0\n",
    "        for c in col1:\n",
    "            if c not in col2 or self.df[c].dtype != xmlstats_obj.df[c].dtype:\n",
    "                errors += 1\n",
    "                if c not in col2:\n",
    "                    str_res += 'problem: column ' + c + ' does not exist\\n'\n",
    "                else:\n",
    "                    str_res += 'problem: columns with the same name ' + c + f' have different types: {self.df[c].dtype}, {xmlstats_obj.df[c].dtype}\\n'\n",
    "        if errors == 0:\n",
    "            str_res += 'Can be merged!\\n'\n",
    "        return str_res, errors==0\n",
    "        \n",
    "    \n",
    "    def merge_xmlstats_objects(self, xmlstats_obj):\n",
    "        if not self.are_mergable(xmlstats_obj):\n",
    "            raise Exception('Can not merge xmlstats objects!')\n",
    "        combined_file_paths = self.file_paths + xmlstats_obj.file_paths\n",
    "        return XML_Stats(combined_file_paths, df=pd.concat([self.df, xmlstats_obj.df]))\n",
    "    \n",
    "    def count_values_in_columns(self):\n",
    "        value_counts = dict(self.df.count())\n",
    "        return [(name, value_counts[name]) for name in (sorted(value_counts, key=value_counts.get, reverse=True))]\n",
    "         \n",
    "    def delete_0_value_columns(self):\n",
    "        \"\"\"\n",
    "        let's delete columns with 0 values ('US Price/Litre', '0/5117')\n",
    "        \"\"\"\n",
    "        res = self.count_values_in_columns()\n",
    "        need_to_delete = []\n",
    "        for column in reversed(res):\n",
    "            name, value = column[0], column[1]\n",
    "            if value != 0:\n",
    "                break\n",
    "            need_to_delete.append(name)\n",
    "        self.delete_columns(need_to_delete)\n",
    "        \n",
    "    # column_names - list of columns names which need to be deleted\n",
    "    def delete_columns(self, column_names):\n",
    "        self.df = self.df.drop(columns=column_names, axis=1, errors='ignore')\n",
    "        \n",
    "    def get_unique_values(self):\n",
    "        res = {}\n",
    "        for c in self.get_columns():\n",
    "            uniques = self.df[c].unique().tolist()\n",
    "            res[c] = (len(uniques), uniques)\n",
    "        return [(name, res[name]) for name in (sorted(res, key=lambda a: res[a][0], reverse=True))]\n",
    "    \n",
    "    def get_unique_values_stats(self):\n",
    "        str_res = ''\n",
    "        uniques = self.get_unique_values()\n",
    "        for u, val in uniques:\n",
    "            if len(val[1]) > 12:\n",
    "                str_res += f'{(u, val[0])}\\n'\n",
    "            else:\n",
    "                str_res += f'{(u, val[0], val[1])}\\n'\n",
    "        return str_res\n",
    "    \n",
    "    def get_not_nan_values_stats(self):\n",
    "        str_res = ''\n",
    "        non_nan_values = self.count_values_in_columns()\n",
    "        ids = str(self.get_lines_amount())\n",
    "        for u, val in non_nan_values:\n",
    "            second_param = str(val)+'/'+ids\n",
    "            str_res += f'{(u, second_param)}\\n'\n",
    "        return str_res\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'file paths: {self.file_paths}, \\nnumber of columns: {self.get_columns_amount()}, \\nnumber of lines: {self.get_lines_amount()}'\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f8e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import pathlib\n",
    "import yaml\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "def get_config():\n",
    "    with open(\"config.yaml\", 'r') as stream:\n",
    "        config = yaml.safe_load(stream)\n",
    "    return config\n",
    "\n",
    "def read_xml_objects(file_dir, file_names):\n",
    "    return [XML_Stats([os.path.join(file_dir, file_name)]) for file_name in file_names]\n",
    "    \n",
    "def write_statistics_to_file(xmlstats_obj, dir_to, file_path=''):\n",
    "    if file_path == '':\n",
    "        file_path = os.path.join(dir_to, pathlib.PurePath(xmlstats_obj.file_paths[0]).name + '.txt')\n",
    "    else:\n",
    "        file_path = os.path.join(dir_to, file_path)\n",
    "    print('print statistics to txt file:', file_path)\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write('\\nCommon info:\\n' + str(xmlstats_obj) + '\\n')\n",
    "        f.write('\\nNumber of unique values per column (uniques values are shown if their amount is less than 12)\\n' + xmlstats_obj.get_unique_values_stats())\n",
    "        f.write('\\nNumber of non nan values per column\\n' + xmlstats_obj.get_not_nan_values_stats())\n",
    "        \n",
    "def write_possible_merging_problems(problems, dir_to):\n",
    "    file_path = os.path.join(dir_to, 'merge_problems.txt')\n",
    "    print('print merging problems to txt file:', file_path)\n",
    "    with open(file_path, 'w') as f:\n",
    "        for line in problems:\n",
    "            f.write('\\n'+line[0]+'\\n')\n",
    "    return file_path   \n",
    "    \n",
    "def merge_xmlstats_objs(xmlstats_objs):\n",
    "    return reduce(lambda obj1, obj2: obj1.merge_xmlstats_objects(obj2), xmlstats_objs)\n",
    "\n",
    "def split_to_list(row):\n",
    "    return row.replace(';', ',')\n",
    "    \n",
    "    \n",
    "def check_flavor_column(obj):\n",
    "    column_name = 'Flavor'\n",
    "    obj.df[column_name] = obj.df[column_name].fillna('')\n",
    "    obj.df[column_name] = obj.df[column_name].apply(lambda row: split_to_list(str(row)))\n",
    "    obj.df[column_name] = obj.df[column_name].astype('string')\n",
    "    \n",
    "def clean_string(row):\n",
    "    row = ''.join(filter(lambda x:x in string.printable, row))\n",
    "    row = re.sub('[\\s+`+*+\\r+\\n+\\\\\\\\+~|^$]', ' ', row)\n",
    "    row = row.replace('\\\\r', ' ')\n",
    "    row = row.replace('\\\\n', ' ')\n",
    "    return row\n",
    "    \n",
    "def clean_string_columns(obj, column_name):\n",
    "    p = re.compile('[\\w\\s\\-&.,/!:;+?_\\'%\\\\(){}\\\"<>=\\[\\]]*')\n",
    "    obj.df[column_name] = obj.df[column_name].fillna('')\n",
    "    obj.df[column_name] = obj.df[column_name].apply(lambda row: clean_string(str(row)))\n",
    "    for val in list(obj.df[column_name]):\n",
    "        if not p.fullmatch(val):\n",
    "            raise Exception('Following structure does not meet predifined metadata: ', val, ' in column ', column_name)\n",
    "    obj.df[column_name] = obj.df[column_name].astype('string')\n",
    "    \n",
    "def clean_float64(row, p):\n",
    "    res = p.findall(str(row))\n",
    "    if len(res) > 0:\n",
    "        return res[0]\n",
    "    # if the value was nan we return nan, not 0.0 (interpolation may be needed)\n",
    "    return row\n",
    "#     return 0.0\n",
    "    \n",
    "def clean_float64_columns(obj, column_name):\n",
    "    p = re.compile(r\"[-+]?(?:\\d*\\.*\\d+)\")\n",
    "    obj.df[column_name] = obj.df[column_name].apply(lambda row: clean_float64(row, p))\n",
    "    obj.df[column_name] = pd.to_numeric(obj.df[column_name])\n",
    "    \n",
    "    \n",
    "def clean_int64(row):\n",
    "    res = re.findall(r'\\d+', str(row))\n",
    "    if len(res) > 0:\n",
    "        return res[0]\n",
    "    # if the value was nan we return nan, not 0 (interpolation may be needed)\n",
    "    return row\n",
    "    \n",
    "def clean_int64_columns(obj, column_name):\n",
    "    obj.df[column_name] = obj.df[column_name].apply(lambda row: clean_int64(row))\n",
    "    obj.df[column_name] = obj.df[column_name].astype('int')\n",
    "    \n",
    "    \n",
    "def clean_data(obj, metadata):\n",
    "    for column, data_type in metadata.items():\n",
    "        # in case the metadata field in config contains column names which don't exist\n",
    "        if column in obj.df.columns:\n",
    "            if data_type == 'string':\n",
    "                clean_string_columns(obj, column)\n",
    "            elif data_type == 'float64':\n",
    "                clean_float64_columns(obj, column)\n",
    "            else:\n",
    "                clean_int64_columns(obj, column)\n",
    "\n",
    "                \n",
    "new_xmlstats_obj = None\n",
    "\n",
    "def main():\n",
    "    config = get_config()\n",
    "    file_dir = config['directory_from']\n",
    "    file_names = config['file_paths']\n",
    "    delete_columns = config['delete_columns']\n",
    "    dir_to = config['directory_to']\n",
    "    metadata = config['metadata']\n",
    "    \n",
    "    #read all files\n",
    "    xmlstats_objs = read_xml_objects(file_dir, file_names)\n",
    "    \n",
    "    #clear a bit\n",
    "    for obj in xmlstats_objs:\n",
    "#         obj.delete_0_value_columns()\n",
    "        obj.delete_columns(delete_columns)\n",
    "        check_flavor_column(obj)\n",
    "        clean_data(obj, metadata)\n",
    "        write_statistics_to_file(obj, dir_to)\n",
    "        \n",
    "    problems_info = [xmlstats_objs[i].get_possible_merging_problems(xmlstats_objs[i+1]) for i in range(len(xmlstats_objs)-1)]\n",
    "    file_path = write_possible_merging_problems(problems_info, dir_to)\n",
    "    for problem in problems_info:\n",
    "        if not problem[1]:\n",
    "            raise Exception('Check ' + file_path + ' file! Data sources can not be merged')\n",
    "\n",
    "    # merge all files into one\n",
    "    global new_xmlstats_obj\n",
    "    new_xmlstats_obj = merge_xmlstats_objs(xmlstats_objs)\n",
    "    write_statistics_to_file(new_xmlstats_obj, dir_to, file_path='cleared_merged_data_stats.txt')\n",
    "    new_xmlstats_obj.write_to_csv(dir_to)\n",
    "    \n",
    "    \n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3675e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plot_of_non_nan_values(obj):\n",
    "    non_nan_values = obj.count_values_in_columns()\n",
    "    x = [pair[0] for pair in non_nan_values]\n",
    "    y = [pair[1] for pair in non_nan_values]\n",
    "    ids = str(obj.get_lines_amount())\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize =(35, 30))\n",
    "    ax.barh(x, y)\n",
    "\n",
    "    for s in ['top', 'bottom', 'left', 'right']:\n",
    "        ax.spines[s].set_visible(False)\n",
    "\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "\n",
    "    ax.xaxis.set_tick_params(pad = 5)\n",
    "    ax.yaxis.set_tick_params(pad = 10)\n",
    "    plt.xticks(fontsize = 20)\n",
    "    plt.yticks(fontsize = 20)\n",
    "\n",
    "    ax.grid(color ='grey', linestyle ='-.', linewidth = 0.5, alpha = 0.2)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Add annotation to bars\n",
    "    for i in ax.patches:\n",
    "        plt.text(i.get_width()+0.2, i.get_y()+0.5,\n",
    "                 str(round((i.get_width()), 2)),\n",
    "                 fontsize = 17, fontweight ='bold',\n",
    "                 color ='grey')\n",
    "    \n",
    "    ax.set_title(\"Non nan values out of \" + str(ids), fontsize = 30)\n",
    "    fig.text(0.9, 0.15, 'Fine Bakery 2023', fontsize = 15,\n",
    "             color ='grey', ha ='right', va ='bottom',\n",
    "             alpha = 0.7)\n",
    "    plt.show()\n",
    " \n",
    "\n",
    "create_plot_of_non_nan_values(new_xmlstats_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2660b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english')).union(set(stopwords.words('french'))).union(set(stopwords.words('dutch'))).union(set(stopwords.words('german'))).union(set(stopwords.words('italian')))\n",
    "\n",
    "def get_key_words_from_product_name(obj, top_amount):\n",
    "    words = []\n",
    "    for product in obj.df['Product Name']:\n",
    "        product = product.lower()\n",
    "        found_words = re.findall(r'\\w+', product)\n",
    "        words += [w for w in found_words if w not in stop_words]\n",
    "    return Counter(words).most_common(top_amount)\n",
    "\n",
    "def get_key_product_names(obj, top_amount):\n",
    "    return dict(obj.df['Product Name'].value_counts().nlargest(top_amount))\n",
    "\n",
    "res1 = get_key_words_from_product_name(new_xmlstats_obj, 50)\n",
    "# print(res1)\n",
    "\n",
    "res2 = get_key_product_names(new_xmlstats_obj, 50)\n",
    "# print(res2)\n",
    "\n",
    "def create_plot(x, y, title):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize =(35, 30))\n",
    "    ax.barh(x, y)\n",
    "\n",
    "    for s in ['top', 'bottom', 'left', 'right']:\n",
    "        ax.spines[s].set_visible(False)\n",
    "\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "\n",
    "    ax.xaxis.set_tick_params(pad = 5)\n",
    "    ax.yaxis.set_tick_params(pad = 10)\n",
    "    plt.xticks(fontsize = 20)\n",
    "    plt.yticks(fontsize = 20)\n",
    "\n",
    "    ax.grid(color ='grey', linestyle ='-.', linewidth = 0.5, alpha = 0.2)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Add annotation to bars\n",
    "    for i in ax.patches:\n",
    "        plt.text(i.get_width()+0.2, i.get_y()+0.5,\n",
    "                 str(round((i.get_width()), 2)),\n",
    "                 fontsize = 17, fontweight ='bold',\n",
    "                 color ='grey')\n",
    "    \n",
    "    ax.set_title(title, fontsize = 30)\n",
    "    fig.text(0.9, 0.15, 'Fine Bakery 2023', fontsize = 15,\n",
    "             color ='grey', ha ='right', va ='bottom',\n",
    "             alpha = 0.7)\n",
    "    plt.show()\n",
    "    \n",
    "x = [pair[0] for pair in res1]\n",
    "y = [pair[1] for pair in res1]\n",
    "create_plot(x, y, \"Top 50 key words\")\n",
    "\n",
    "create_plot(list(res2.keys()), list(res2.values()), \"Top 50 key product names\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ec5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def get_all_words(obj):\n",
    "    words = ''\n",
    "    for product in obj.df['Product Name']:\n",
    "        product = product.lower()\n",
    "        found_words = re.findall(r'\\w+', product)\n",
    "        cleaned_words = [w for w in found_words if w not in stop_words]\n",
    "        words += \" \".join(word for word in cleaned_words)\n",
    "    return words\n",
    "\n",
    "text = get_all_words(new_xmlstats_obj)\n",
    "\n",
    "# create word cloud\n",
    "# see all possible colormaps at https://www.kaggle.com/code/niteshhalai/wordcloud-colormap\n",
    "word_cloud = WordCloud(width = 2500, height = 1500, collocations = False, colormap=\"hsv\").generate(text)\n",
    "\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"word_cloud_bakery.png\", format=\"png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba16797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
